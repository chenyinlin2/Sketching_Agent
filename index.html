<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <!-- Meta tags for social media banners, these should be filled in appropriatly as they are your "business card" -->
  <!-- Replace the content tag with appropriate information -->
  <meta name="description" content="DESCRIPTION META TAG">
  <meta property="og:title" content="SOCIAL MEDIA TITLE TAG"/>
  <meta property="og:description" content="SOCIAL MEDIA DESCRIPTION TAG TAG"/>
  <meta property="og:url" content="URL OF THE WEBSITE"/>
  <!-- Path to banner image, should be in the path listed below. Optimal dimenssions are 1200X630-->
  <meta property="og:image" content="static/image/your_banner_image.png" />
  <meta property="og:image:width" content="1200"/>
  <meta property="og:image:height" content="630"/>


  <meta name="twitter:title" content="TWITTER BANNER TITLE META TAG">
  <meta name="twitter:description" content="TWITTER BANNER DESCRIPTION META TAG">
  <!-- Path to banner image, should be in the path listed below. Optimal dimenssions are 1200X600-->
  <meta name="twitter:image" content="static/images/your_twitter_banner_image.png">
  <meta name="twitter:card" content="summary_large_image">
  <!-- Keywords for your paper to be indexed by-->
  <meta name="keywords" content="KEYWORDS SHOULD BE PLACED HERE">
  <meta name="viewport" content="width=device-width, initial-scale=1">


  <title>Sketching Agent: Reconstruct sketches with human-like concise strokes based on Constrained Markov Decision Process</title>
  <link rel="icon" type="image/x-icon" href="static/images/favicon.ico">
  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro"
  rel="stylesheet">

  <link rel="stylesheet" href="static/css/bulma.min.css">
  <link rel="stylesheet" href="static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="static/css/fontawesome.all.min.css">
  <link rel="stylesheet"
  href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="static/css/index.css">

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script src="https://documentcloud.adobe.com/view-sdk/main.js"></script>
  <script defer src="static/js/fontawesome.all.min.js"></script>
  <script src="static/js/bulma-carousel.min.js"></script>
  <script src="static/js/bulma-slider.min.js"></script>
  <script src="static/js/index.js"></script>
  <style>
    .dashed-arrow {
      display: inline-block;
      width: 25px;     /* 箭头主体长度 */
      height: 2px;
      background: none;
      border-bottom: 2px dashed currentColor;
      position: relative;
      vertical-align: middle;
      /* —— 在这里增加左右空格 —— */
      margin: 0 0.2em;  /* 上下 0，左右各 0.5em */
    }
    .dashed-arrow::after {
      content: '';
      position: absolute;
      right: -6px;      /* 箭头大小 */
      top: -4px;
      border-top: 6px solid transparent;
      border-bottom: 6px solid transparent;
      border-left: 6px solid currentColor;
    }
  </style>
</head>
<body>
  <section class="hero">
    <div class="hero-body">
      <div class="container is-max-desktop">
        <div class="columns is-centered">
          <div class="column has-text-centered">
            <h1 class="title is-1 publication-title">Sketching Agent: Reconstruct sketches with human-like concise strokes based on Constrained Markov Decision Process</h1>
            <div class="is-size-5 publication-authors">
              <!-- Paper authors -->
              <span class="author-block">
                Gaofeng Liu<sup>a</sup>,</span>
              <span class="author-block">
                Jian Liu<sup>b</sup>,</span>
              <span class="author-block">
                Yongqi Shao<sup>a</sup>,</span>
              <span class="author-block">
                Xuetong Li<sup>a</sup>,</span>
              <span class="author-block">
                Hong Huo<sup>a</sup>,</span>
              <span class="author-block">
                Tao Fang<sup>a*</sup></span>
            </div>
                  <div class="is-size-5 publication-authors">
                    <span class="author-block"><sup>a</sup> Shanghai Jiao Tong University</span>
                  </div>
                  <div class="is-size-5 publication-authors">
                    <span class="author-block"><sup>b</sup> University of Shanghai for Science and Technology</span>
                  </div>
                  <div class="column has-text-centered">
                    <div class="publication-links">
                   
                  <!-- Github link -->
                  <span class="link-block">
                    <a href="https://github.com/chenyinlin2/Sketching_Agent_code" target="_blank"
                    class="external-link button is-normal is-rounded is-dark">
                    <span class="icon">
                      <i class="fab fa-github"></i>
                    </span>
                    <span>Code</span>
                  </a>
                </span>

            </div>
          </div>
        </div>
      </div>
    </div>
  </div>
</section>


<!-- Teaser video-->
<section class="hero teaser">
  <div class="container is-max-desktop">
    <div class="hero-body">
      <img src="static/img/inference_process.png" alt="Banner Image" id="tree" height="100%">
      <h2 class="subtitle has-text-centered">
        Figure 1. Overview of Sketching Agent drawing a target sketch. In the inference process, the agent outputs stroke parameters according to the current start point and the canvas at each time. The renderer maps the strokes to the canvas. The <span class="dashed-arrow"></span> indicates that the end point of the current stroke serves as the start point for the next stroke.
      </h2>
    </div>
  </div>
</section>
<!-- End teaser video -->

<!-- Paper abstract -->
<section class="section hero is-light">
  <div class="container is-max-desktop">
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Abstract</h2>
        <div class="content has-text-justified">
          <p>
            Sketch reconstruction aims to recreate a target sketch by generating a sequence of vector strokes. Traditional methods often focus solely on the visual similarity of the final drawing while neglecting the stroke generation process, resulting in redundant strokes and disordered sequences. To address this limitation, we propose a sketch agent framework based on a Constrained Markov Decision Process (CMDP). To ensure the spatial continuity between adjacent strokes and get closer to the human drawing process, we introduce a hybrid action space for the sketching agent. Furthermore, we carefully design reward and cost functions to guide the agent in achieving efficient sketch reconstruction using more concise strokes while maintaining visual fidelity. Unlike existing methods that rely on supervised learning, our framework adopts a self-supervised learning paradigm, freeing it from the dependence on paired vector label data. Experimental results on the MNIST and QuickDraw datasets demonstrate the significant advantages of our approach in various sketch reconstruction tasks. Ablation studies further validate the effectiveness of our method in reducing the number of strokes and optimizing their sequence.
          </p>
        </div>
      </div>
    </div>
  </div>
</section>
<!-- End paper abstract -->

<!-- Single image display -->
<section class="hero is-small">
  <div class="hero-body">
    <div class="container">
      <!-- Paper video. -->
      <h2 class="title is-3">Method</h2>
      <h2 class="content has-text-justified">
        In this study, we explore how to simulate humans and reconstruct the target sketch with more concise strokes and better order. Traditional methods focus more on the sketch reconstruction results and ignore the sketch reconstruction process, which makes the generated strokes have a messy order and redundant number. The sketch agent we proposed is a novel framework based on constrained Markov decision process, which can decompose the target sketch into more concise continuous vector strokes. Moreover, the sketch agent is based on self-supervised learning of pixel-level sketches, without the need for vector label data pairs. At the initial stage, the sketch agent starts drawing from the upper left corner. Instead of drawing strokes directly on the canvas, the agent relies on the information of the current canvas and the starting point to determine the stroke parameters that can be used to draw strokes at a specific moment during the reasoning process. The renderer converts the one-dimensional stroke parameters into a two-dimensional image and uses the end point of the current stroke as the starting point of the next stroke. Fig.2 shows multiple iterations of this process, which ultimately results in the reconstruction of a complete sketch. In order to maximize the cumulative reward while following the constraints, the sketch drawing agent chooses the appropriate operation at each step. This enables the agent to draw longer strokes and overlap the original image as much as possible, thus simplifying the sketch reconstruction process. 
      </h2>
      <figure class="hero-body">
        <img src="static/img/strokes_during_inference.png" alt="strokes_during_inference">
      </figure>
      <h2 class="subtitle has-text-centered">
        Figure 2. During the sketch reconstruction, the strokes and canvas at each moment. The top line represents the sequence of strokes, while the bottom line indicates the strokes that have already been rendered on the canvas as of the current time. The red and yellow dots represent the start point and end point of the stroke respectively.
      </h2>

      <figure class="hero-body">
        <img src="static/img/actor_critic_network.png" alt="actor_critic_network">
      </figure>
      <h2 class="subtitle has-text-centered">
        Figure 3. The architecture of the actor network and the critic network.
      </h2>

      <figure class="hero-body">
        <img src="static/img/training_framework.png" alt="training_framework">
      </figure>
      <h2 class="subtitle has-text-centered">
        Figure 4. The training framework of Sketching Agent.
      </h2>
    </div>


  </div>
</section>
<!-- Single image display -->
 
<!-- Youtube video -->
<section class="hero is-small is-light">
  <div class="hero-body">
    <div class="container">
      <!-- Paper video. -->
      <h2 class="title is-3">Experiments</h2>
      <div class="content has-text-justified">
        <p>
          We evaluate our Sketching Agent on two datasets: MNIST and QuickDraw. MNIST comprises 70 000 handwritten digits (60 000 for training, 10 000 for testing), each a 28×28 grayscale image. QuickDraw contains 50 million sketches across 345 categories; we randomly sample 50 000 for training, 5 000 for validation, and 5 000 for testing, discarding category labels. All images are resized to 128×128; QuickDraw sketches are further augmented by scaling to 256×256 and extracting four random 128×128 crops. Both the actor and critic networks use ADAM (initial learning rates 1 × 10⁻⁴ and 1 × 10⁻³, step‐decay scheduler), a batch size of 48, and a replay buffer of 40 000. We train for 40 000 episodes on MNIST (max 10 steps per episode) and 20 000 on QuickDraw (max 40 steps), with discount factor γ=0.95. On a single NVIDIA 3090 Ti, training takes ~7 h for MNIST and ~48 h for QuickDraw. Episodes terminate when the step limit is reached or the pen is lifted twice consecutively.
        </p>
      </div>
    </div>
  </div>
</section>
<!-- End youtube video -->

<!-- Video carousel -->
<section class="hero is-small">
  <div class="hero-body">
    <div class="container">
      <h2 class="title is-3">Results</h2>

        <div class="grid-container">
          <img src="static\img_gif\1014_1.GIF" alt="GIF 1">
          <img src="static\img_gif\1014_2.GIF" alt="GIF 2">
          <img src="static\img_gif\1014_gt.jpg" alt="GIF 2">
            
          <img src="static\img_gif\277_move_trajectory_with_point_only_down.gif" alt="GIF 3">
          <img src="static\img_gif\277_move_color_trajectory_with_point_only_down.gif" alt="GIF 4">
          <img src="static\img_gif\277_gt.jpg" alt="GIF 4">

          <img src="static\img_gif\2013_move_trajectory_with_point_only_down.gif" alt="GIF 3">
          <img src="static\img_gif\2013_move_color_trajectory_with_point_only_down.gif" alt="GIF 4">
          <img src="static\img_gif\2013_gt.jpg" alt="GIF 4">

          <img src="static\img_gif\2092_move_trajectory_with_point_only_down.gif" alt="GIF 3">
          <img src="static\img_gif\2092_move_color_trajectory_with_point_only_down.gif" alt="GIF 4">
          <img src="static\img_gif\2092_gt.jpg" alt="GIF 4">

          <img src="static\img_gif\236_move_trajectory_with_point_only_down.gif" alt="GIF 3">
          <img src="static\img_gif\236_move_color_trajectory_with_point_only_down.gif" alt="GIF 4">
          <img src="static\img_gif\236_gt.jpg" alt="GIF 4">

          <img src="static\img_gif\51_move_trajectory_with_point_only_down.gif" alt="GIF 3">
          <img src="static\img_gif\51_move_color_trajectory_with_point_only_down.gif" alt="GIF 4">
          <img src="static\img_gif\51_gt.jpg" alt="GIF 4">
          
          <img src="static\img_gif\174_move_trajectory_with_point_only_down.gif" alt="GIF 3">
          <img src="static\img_gif\174_move_color_trajectory_with_point_only_down.gif" alt="GIF 4">
          <img src="static\img_gif\174_gt.jpg" alt="GIF 4">

          <img src="static\img_gif\266_move_trajectory_with_point_only_down.gif" alt="GIF 3">
          <img src="static\img_gif\266_move_color_trajectory_with_point_only_down.gif" alt="GIF 4">
          <img src="static\img_gif\266_gt.jpg" alt="GIF 4">

          <img src="static\img_gif\278_move_trajectory_with_point_only_down.gif" alt="GIF 3">
          <img src="static\img_gif\278_move_color_trajectory_with_point_only_down.gif" alt="GIF 4">
          <img src="static\img_gif\278_gt.jpg" alt="GIF 4">

          <!-- <img src="static\img_gif\2036_move_trajectory_with_point_only_down.gif" alt="GIF 3">
          <img src="static\img_gif\2036_move_color_trajectory_with_point_only_down.gif" alt="GIF 4">
          <img src="static\img_gif\2036_gt.jpg" alt="GIF 4"> -->

          <img src="static\img_gif\2123_move_trajectory_with_point_only_down.gif" alt="GIF 3">
          <img src="static\img_gif\2123_move_color_trajectory_with_point_only_down.gif" alt="GIF 4">
          <img src="static\img_gif\2123_gt.jpg" alt="GIF 4">

          <img src="static\minist_gif\61_move_trajectory_with_point_only_down.gif" alt="GIF 3">
          <img src="static\minist_gif\61_move_color_trajectory_with_point_only_down.gif" alt="GIF 4">
          <img src="static\minist_gif\61_gt.jpg" alt="GIF 4">

          <img src="static\minist_gif\120_move_trajectory_with_point_only_down.gif" alt="GIF 3">
          <img src="static\minist_gif\120_move_color_trajectory_with_point_only_down.gif" alt="GIF 4">
          <img src="static\minist_gif\120_gt.jpg" alt="GIF 4">

          <img src="static\minist_gif\56_move_trajectory_with_point_only_down.gif" alt="GIF 3">
          <img src="static\minist_gif\56_move_color_trajectory_with_point_only_down.gif" alt="GIF 4">
          <img src="static\minist_gif\56_gt.png" alt="GIF 4">

          <img src="static\minist_gif\481_move_trajectory_with_point_only_down.gif" alt="GIF 3">
          <img src="static\minist_gif\481_move_color_trajectory_with_point_only_down.gif" alt="GIF 4">
          <img src="static\minist_gif\481_gt.png" alt="GIF 4">

          <img src="static\minist_gif\528_move_trajectory_with_point_only_down.gif" alt="GIF 3">
          <img src="static\minist_gif\528_move_color_trajectory_with_point_only_down.gif" alt="GIF 4">
          <img src="static\minist_gif\528_gt.png" alt="GIF 4">

          <img src="static\minist_gif\545_move_trajectory_with_point_only_down.gif" alt="GIF 3">
          <img src="static\minist_gif\545_move_color_trajectory_with_point_only_down.gif" alt="GIF 4">
          <img src="static\minist_gif\545_gt.png" alt="GIF 4">
        </div>

        <figure class="hero-body">
          <img src="static/img/different_dataset_results.png" alt="different_dataset_results">
        </figure>
        <h2 class="subtitle has-text-centered">
          Figure 5. The sketching process and results on different datasets. Three columns from left to right represent the original image, the results of drawing contour, and stroke order, respectively.
        </h2>
        <div class="content has-text-justified">
          <p>
            Vector-Line-Art is tailored for line-art images, while Learning‑To‑Paint is designed for complex images. Therefore, we conducted comparative evaluations of these two methods on the QuickDraw and MNIST datasets. Figure 6 shows that, compared to Learning‑To‑Paint, the Sketching Agent can reconstruct sketches using continuous, human‑like strokes.
          </p>
        </div>
        <figure class="hero-body">
          <img src="static/img/campare_with_LTP.png" alt="campare_with_LTP">
        </figure>
        <h2 class="subtitle has-text-centered">
          Figure 6. Comparison with Learning-To-Paint on MNIST. Row (a) is the strokes sequence generated by Learning-To-Paint model, and Row (b) is the strokes sequence generated by our model.
        </h2>

        <div class="content has-text-justified">
          <p>
            Figure 7 demonstrates that our method, by employing more succinct continuous strokes, reconstructs the target image while preserving the same visual appearance.
          </p>
        </div>
        
        <figure class="hero-body">
          <img src="static/img/campare_with_Vector-Line-Art.png" alt="campare_with_Vector-Line-Art">
        </figure>
        <h2 class="subtitle has-text-centered">
          Figure 7. Comparison with Vector-Line-Art. Column (a) represents the target image. Columns (b) and (c) represent the results of our Sketching Agent and Vector-Line-Art, respectively. In columns (b) and (c), from left to right are the drawing results and the moving trajectory.
        </h2>
        <figure class="hero-body">
          <img src="static/img/quantitative_comparison.png" alt="quantitative_comparison">
        </figure>
        <h2 class="subtitle has-text-centered">
          Table 1. Quantitative comparison on QuickDraw and MNIST.
        </h2>
    </div>
  </div>
</section>
<!-- End video carousel -->

<!-- Video carousel -->
<section class="hero is-small is-light">
  <div class="hero-body">
    <div class="container">
      <h2 class="title is-3">Ablation Study</h2>
      <p>
        Results of ablation experiments based on QuickDraw.
      </p>
        <figure class="hero-body">
          <img src="static/img/impact_of_status_reward.png" alt="impact_of_status_reward">
        </figure>
        <h2 class="subtitle has-text-centered">
          Figure 9. Impact of Status Reward/Cost on agent policies. The first column represents the target images. The second column is the result with Status Reward. The colored strokes show the movement trajectory during the reconstruction.
        </h2>

        <figure class="hero-body">
          <img src="static/img/impact_of_end_point_reward.png" alt="impact_of_end_point_reward">
        </figure>
        <h2 class="subtitle has-text-centered">
          Figure 10. Impact of End Point Reward on agent policy. The output is the result with Stroke End Reward, and output* represents the result without Stroke End Reward.
        </h2>

        <figure class="hero-body">
          <img src="static/img/ablation_experiment_quantitative_metrics.png" alt="ablation_experiment_quantitative_metrics">
        </figure>
        <h2 class="subtitle has-text-centered">
          Figure 11. Comparison of quantitative metrics in ablation experiment.
        </h2>
    </div>
  </div>
</section>
<!-- End video carousel -->

  <footer class="footer">
  <div class="container">
    <div class="columns is-centered">
      <div class="column is-8">
        <div class="content">
          <p>
            This page was built using the <a href="https://github.com/eliahuhorwitz/Academic-project-page-template" target="_blank">Academic Project Page Template</a> which was adopted from the <a href="https://nerfies.github.io" target="_blank">Nerfies</a> project page.
            You are free to borrow the source code of this website, we just ask that you link back to this page in the footer. <br> This website is licensed under a <a rel="license"  href="http://creativecommons.org/licenses/by-sa/4.0/" target="_blank">Creative
            Commons Attribution-ShareAlike 4.0 International License</a>.
          </p>
        </div>
      </div>
    </div>
  </div>
</footer>

<!-- Statcounter tracking code -->
  
<!-- You can add a tracker to track page visits by creating an account at statcounter.com -->

    <!-- End of Statcounter Code -->

  </body>
  </html>
